{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c0a7808",
   "metadata": {},
   "source": [
    "# Module 1 — Prévision BVMT\n",
    "\n",
    "**Objectifs:**\n",
    "- Prédire le prix de clôture des 5 prochains jours ouvrables\n",
    "- Prédire le volume journalier et la probabilité de liquidité élevée/faible\n",
    "\n",
    "**Métriques:** RMSE, MAE, Directional Accuracy\n",
    "\n",
    "**Pipeline:** Data Loading → Feature Engineering → Model Training → Evaluation → Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9cf571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b0ab0",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "Load all historical cotation files from the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join('..', 'data')\n",
    "\n",
    "def load_csv_files(data_dir):\n",
    "    \"\"\"Load all CSV cotation files.\"\"\"\n",
    "    csv_files = sorted(glob.glob(os.path.join(data_dir, 'histo_cotation_*.csv')))\n",
    "    dfs = []\n",
    "    for f in csv_files:\n",
    "        df = pd.read_csv(f, sep=';', encoding='latin-1')\n",
    "        # Strip whitespace from column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "        dfs.append(df)\n",
    "        print(f\"Loaded {os.path.basename(f)}: {len(df)} rows\")\n",
    "    return dfs\n",
    "\n",
    "def load_txt_files(data_dir):\n",
    "    \"\"\"Load older fixed-width TXT cotation files.\"\"\"\n",
    "    txt_files = sorted(glob.glob(os.path.join(data_dir, 'histo_cotation_*.txt')))\n",
    "    dfs = []\n",
    "    for f in txt_files:\n",
    "        df = pd.read_fwf(f, encoding='latin-1', skiprows=[1])  # skip the dashes line\n",
    "        df.columns = df.columns.str.strip()\n",
    "        # Rename columns to match CSV format if needed\n",
    "        rename = {'IND_RES': 'IND_RES'}\n",
    "        df = df.rename(columns=rename)\n",
    "        dfs.append(df)\n",
    "        print(f\"Loaded {os.path.basename(f)}: {len(df)} rows\")\n",
    "    return dfs\n",
    "\n",
    "csv_dfs = load_csv_files(DATA_DIR)\n",
    "txt_dfs = load_txt_files(DATA_DIR)\n",
    "\n",
    "# Combine all DataFrames\n",
    "all_dfs = txt_dfs + csv_dfs\n",
    "\n",
    "# Standardize columns across all DataFrames\n",
    "standard_cols = ['SEANCE', 'GROUPE', 'CODE', 'VALEUR', 'OUVERTURE', 'CLOTURE',\n",
    "                 'PLUS_BAS', 'PLUS_HAUT', 'QUANTITE_NEGOCIEE', 'NB_TRANSACTION', 'CAPITAUX']\n",
    "\n",
    "cleaned_dfs = []\n",
    "for df in all_dfs:\n",
    "    # Keep only standard columns that exist\n",
    "    available = [c for c in standard_cols if c in df.columns]\n",
    "    cleaned_dfs.append(df[available])\n",
    "\n",
    "data = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "print(f\"\\nTotal combined rows: {len(data)}\")\n",
    "print(f\"Columns: {list(data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c879c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare the data\n",
    "\n",
    "# Strip whitespace from string columns\n",
    "for col in ['CODE', 'VALEUR']:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].astype(str).str.strip()\n",
    "\n",
    "# Parse dates\n",
    "data['SEANCE'] = pd.to_datetime(data['SEANCE'].astype(str).str.strip(), format='%d/%m/%Y', errors='coerce')\n",
    "data = data.dropna(subset=['SEANCE'])\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_cols = ['OUVERTURE', 'CLOTURE', 'PLUS_BAS', 'PLUS_HAUT', 'QUANTITE_NEGOCIEE', 'NB_TRANSACTION', 'CAPITAUX']\n",
    "for col in numeric_cols:\n",
    "    if col in data.columns:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Sort by date\n",
    "data = data.sort_values(['CODE', 'SEANCE']).reset_index(drop=True)\n",
    "\n",
    "# Remove rows with zero or null closing price\n",
    "data = data[(data['CLOTURE'] > 0) & data['CLOTURE'].notna()]\n",
    "\n",
    "print(f\"Cleaned data: {len(data)} rows\")\n",
    "print(f\"Date range: {data['SEANCE'].min()} to {data['SEANCE'].max()}\")\n",
    "print(f\"Number of stocks: {data['CODE'].nunique()}\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860829c",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9eb14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 most traded stocks (by number of sessions)\n",
    "stock_counts = data.groupby(['CODE', 'VALEUR']).agg(\n",
    "    sessions=('SEANCE', 'count'),\n",
    "    avg_volume=('QUANTITE_NEGOCIEE', 'mean'),\n",
    "    avg_close=('CLOTURE', 'mean')\n",
    ").sort_values('sessions', ascending=False)\n",
    "\n",
    "top_stocks = stock_counts.head(15)\n",
    "print(\"Top 15 stocks by number of trading sessions:\")\n",
    "top_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b2aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a representative stock to demonstrate the pipeline\n",
    "# We'll use the stock with the most data points\n",
    "DEMO_CODE = stock_counts.index[0][0]\n",
    "DEMO_NAME = stock_counts.index[0][1]\n",
    "print(f\"Demo stock: {DEMO_NAME} ({DEMO_CODE})\")\n",
    "\n",
    "stock_data = data[data['CODE'] == DEMO_CODE].copy()\n",
    "stock_data = stock_data.set_index('SEANCE').sort_index()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "axes[0].plot(stock_data.index, stock_data['CLOTURE'], linewidth=0.8, color='#2563eb')\n",
    "axes[0].set_title(f'{DEMO_NAME} — Prix de Clôture', fontweight='bold')\n",
    "axes[0].set_ylabel('Prix (TND)')\n",
    "\n",
    "axes[1].bar(stock_data.index, stock_data['QUANTITE_NEGOCIEE'], width=2, color='#10b981', alpha=0.7)\n",
    "axes[1].set_title('Volume Négocié', fontweight='bold')\n",
    "axes[1].set_ylabel('Quantité')\n",
    "\n",
    "axes[2].plot(stock_data.index, stock_data['CAPITAUX'], linewidth=0.8, color='#f59e0b')\n",
    "axes[2].set_title('Capitaux', fontweight='bold')\n",
    "axes[2].set_ylabel('Capitaux (TND)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae5e84",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "We build a rich feature set for each stock's time series:\n",
    "- **Lagged prices**: previous N days closing prices\n",
    "- **Moving averages**: 5, 10, 20, 50 day\n",
    "- **Volatility**: rolling standard deviation\n",
    "- **RSI** (Relative Strength Index)\n",
    "- **Bollinger Bands** width\n",
    "- **Volume features**: rolling mean, ratio\n",
    "- **Returns**: daily, cumulative\n",
    "- **Calendar**: day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rsi(series, period=14):\n",
    "    \"\"\"Compute Relative Strength Index.\"\"\"\n",
    "    delta = series.diff()\n",
    "    gain = delta.where(delta > 0, 0.0)\n",
    "    loss = -delta.where(delta < 0, 0.0)\n",
    "    avg_gain = gain.rolling(window=period, min_periods=period).mean()\n",
    "    avg_loss = loss.rolling(window=period, min_periods=period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def engineer_features(df, lookback=30):\n",
    "    \"\"\"\n",
    "    Engineer features for a single stock's DataFrame.\n",
    "    Expects columns: CLOTURE, OUVERTURE, PLUS_HAUT, PLUS_BAS, QUANTITE_NEGOCIEE, CAPITAUX, NB_TRANSACTION\n",
    "    Index should be datetime (SEANCE).\n",
    "    \"\"\"\n",
    "    feat = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # ── Price features ──\n",
    "    feat['close'] = df['CLOTURE']\n",
    "    feat['open'] = df['OUVERTURE']\n",
    "    feat['high'] = df['PLUS_HAUT']\n",
    "    feat['low'] = df['PLUS_BAS']\n",
    "    feat['volume'] = df['QUANTITE_NEGOCIEE']\n",
    "\n",
    "    # Daily return\n",
    "    feat['return_1d'] = feat['close'].pct_change()\n",
    "    feat['return_5d'] = feat['close'].pct_change(5)\n",
    "    feat['return_10d'] = feat['close'].pct_change(10)\n",
    "\n",
    "    # Lagged closing prices\n",
    "    for lag in [1, 2, 3, 5, 10, 20]:\n",
    "        feat[f'close_lag_{lag}'] = feat['close'].shift(lag)\n",
    "\n",
    "    # Moving averages\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        feat[f'ma_{window}'] = feat['close'].rolling(window).mean()\n",
    "        feat[f'ma_ratio_{window}'] = feat['close'] / feat[f'ma_{window}']\n",
    "\n",
    "    # Exponential moving averages\n",
    "    for span in [12, 26]:\n",
    "        feat[f'ema_{span}'] = feat['close'].ewm(span=span).mean()\n",
    "\n",
    "    # MACD\n",
    "    feat['macd'] = feat['ema_12'] - feat['ema_26']\n",
    "    feat['macd_signal'] = feat['macd'].ewm(span=9).mean()\n",
    "    feat['macd_hist'] = feat['macd'] - feat['macd_signal']\n",
    "\n",
    "    # Volatility\n",
    "    feat['volatility_5'] = feat['return_1d'].rolling(5).std()\n",
    "    feat['volatility_20'] = feat['return_1d'].rolling(20).std()\n",
    "\n",
    "    # RSI\n",
    "    feat['rsi_14'] = compute_rsi(feat['close'], 14)\n",
    "\n",
    "    # Bollinger Bands\n",
    "    bb_ma = feat['close'].rolling(20).mean()\n",
    "    bb_std = feat['close'].rolling(20).std()\n",
    "    feat['bb_upper'] = bb_ma + 2 * bb_std\n",
    "    feat['bb_lower'] = bb_ma - 2 * bb_std\n",
    "    feat['bb_width'] = (feat['bb_upper'] - feat['bb_lower']) / bb_ma\n",
    "    feat['bb_position'] = (feat['close'] - feat['bb_lower']) / (feat['bb_upper'] - feat['bb_lower'])\n",
    "\n",
    "    # Volume features\n",
    "    feat['volume_ma_5'] = feat['volume'].rolling(5).mean()\n",
    "    feat['volume_ma_20'] = feat['volume'].rolling(20).mean()\n",
    "    feat['volume_ratio'] = feat['volume'] / feat['volume_ma_20']\n",
    "\n",
    "    # Candlestick patterns\n",
    "    feat['body'] = feat['close'] - feat['open']\n",
    "    feat['body_pct'] = feat['body'] / feat['open']\n",
    "    feat['upper_shadow'] = feat['high'] - feat[['close', 'open']].max(axis=1)\n",
    "    feat['lower_shadow'] = feat[['close', 'open']].min(axis=1) - feat['low']\n",
    "\n",
    "    # Calendar features\n",
    "    feat['day_of_week'] = df.index.dayofweek\n",
    "    feat['month'] = df.index.month\n",
    "\n",
    "    # Capitaux\n",
    "    feat['capitaux'] = df['CAPITAUX']\n",
    "    feat['capitaux_ma_10'] = feat['capitaux'].rolling(10).mean()\n",
    "\n",
    "    return feat\n",
    "\n",
    "\n",
    "# Apply to the demo stock\n",
    "features = engineer_features(stock_data)\n",
    "print(f\"Features shape: {features.shape}\")\n",
    "print(f\"\\nFeature columns ({len(features.columns)}):\")\n",
    "print(list(features.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842d1b7c",
   "metadata": {},
   "source": [
    "## 4. Prepare Training Data for Multi-Step Price Forecasting\n",
    "\n",
    "We predict the closing price for the next **5 business days** (t+1 to t+5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORECAST_HORIZON = 5  # predict next 5 trading days\n",
    "\n",
    "def create_targets(df, horizon=FORECAST_HORIZON):\n",
    "    \"\"\"\n",
    "    Create target columns: future closing prices at t+1, t+2, ..., t+horizon.\n",
    "    \"\"\"\n",
    "    targets = pd.DataFrame(index=df.index)\n",
    "    for h in range(1, horizon + 1):\n",
    "        targets[f'target_close_t{h}'] = df['close'].shift(-h)\n",
    "        targets[f'target_volume_t{h}'] = df['volume'].shift(-h)\n",
    "    return targets\n",
    "\n",
    "\n",
    "targets = create_targets(features)\n",
    "\n",
    "# Combine features + targets and drop NaN rows\n",
    "full_df = pd.concat([features, targets], axis=1).dropna()\n",
    "print(f\"Full dataset shape (after dropping NaN): {full_df.shape}\")\n",
    "print(f\"Date range: {full_df.index.min()} → {full_df.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (exclude raw close/open/high/low/volume and targets)\n",
    "target_cols = [c for c in full_df.columns if c.startswith('target_')]\n",
    "price_target_cols = [c for c in target_cols if 'close' in c]\n",
    "volume_target_cols = [c for c in target_cols if 'volume' in c]\n",
    "\n",
    "# Features for XGBoost (all numeric features except targets)\n",
    "feature_cols = [c for c in full_df.columns if c not in target_cols]\n",
    "\n",
    "print(f\"Feature columns: {len(feature_cols)}\")\n",
    "print(f\"Price target columns: {price_target_cols}\")\n",
    "print(f\"Volume target columns: {volume_target_cols}\")\n",
    "\n",
    "# Train/test split — time-based (last 60 trading days = ~3 months as test)\n",
    "TEST_DAYS = 60\n",
    "\n",
    "X = full_df[feature_cols]\n",
    "y_price = full_df[price_target_cols]\n",
    "y_volume = full_df[volume_target_cols]\n",
    "\n",
    "X_train, X_test = X.iloc[:-TEST_DAYS], X.iloc[-TEST_DAYS:]\n",
    "y_price_train, y_price_test = y_price.iloc[:-TEST_DAYS], y_price.iloc[-TEST_DAYS:]\n",
    "y_volume_train, y_volume_test = y_volume.iloc[:-TEST_DAYS], y_volume.iloc[-TEST_DAYS:]\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train)} samples ({X_train.index.min()} → {X_train.index.max()})\")\n",
    "print(f\"Test set:  {len(X_test)} samples ({X_test.index.min()} → {X_test.index.max()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a197dc8f",
   "metadata": {},
   "source": [
    "## 5. Model Training — XGBoost (Price Forecasting)\n",
    "\n",
    "Train one XGBoost regressor per forecast horizon (direct multi-step strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd466293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost models for each horizon\n",
    "xgb_models = {}\n",
    "\n",
    "for h in range(1, FORECAST_HORIZON + 1):\n",
    "    target_col = f'target_close_t{h}'\n",
    "    print(f\"\\nTraining XGBoost for t+{h} ...\")\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        min_child_weight=5,\n",
    "        reg_alpha=0.1,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        early_stopping_rounds=30,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_price_train[target_col],\n",
    "        eval_set=[(X_test, y_price_test[target_col])],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    xgb_models[h] = model\n",
    "\n",
    "    # Quick evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_price_test[target_col], y_pred))\n",
    "    mae = mean_absolute_error(y_price_test[target_col], y_pred)\n",
    "    print(f\"  t+{h}: RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
    "\n",
    "print(\"\\n✓ All XGBoost price models trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9873d00",
   "metadata": {},
   "source": [
    "## 6. Model Training — LSTM (Price Forecasting)\n",
    "\n",
    "Compare with an LSTM model using the same features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b271aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for LSTM\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_scaled = scaler_y.fit_transform(y_price_train)\n",
    "y_test_scaled = scaler_y.transform(y_price_test)\n",
    "\n",
    "# Reshape for LSTM: (samples, timesteps=1, features)\n",
    "# We use a sliding window of LOOKBACK days\n",
    "LOOKBACK = 20\n",
    "\n",
    "def create_lstm_sequences(X, y, lookback=LOOKBACK):\n",
    "    \"\"\"Create sequences for LSTM from 2D feature arrays.\"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(lookback, len(X)):\n",
    "        Xs.append(X[i - lookback:i])\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "X_train_lstm, y_train_lstm = create_lstm_sequences(X_train_scaled, y_train_scaled)\n",
    "X_test_lstm, y_test_lstm = create_lstm_sequences(X_test_scaled, y_test_scaled)\n",
    "\n",
    "print(f\"LSTM train shape: X={X_train_lstm.shape}, y={y_train_lstm.shape}\")\n",
    "print(f\"LSTM test shape:  X={X_test_lstm.shape}, y={y_test_lstm.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a27fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "n_features = X_train_lstm.shape[2]\n",
    "n_outputs = FORECAST_HORIZON  # 5 days\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(LOOKBACK, n_features)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(n_outputs)\n",
    "])\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "history = lstm_model.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    validation_data=(X_test_lstm, y_test_lstm),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_title('Loss (MSE)')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(history.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_title('MAE')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb43c28",
   "metadata": {},
   "source": [
    "## 7. Volume & Liquidity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f14a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume Prediction — XGBoost\n",
    "volume_models = {}\n",
    "\n",
    "for h in range(1, FORECAST_HORIZON + 1):\n",
    "    target_col = f'target_volume_t{h}'\n",
    "    print(f\"Training volume XGBoost for t+{h} ...\")\n",
    "\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        verbosity=0,\n",
    "        early_stopping_rounds=20,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_volume_train[target_col],\n",
    "        eval_set=[(X_test, y_volume_test[target_col])],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    volume_models[h] = model\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_volume_test[target_col], y_pred))\n",
    "    mae = mean_absolute_error(y_volume_test[target_col], y_pred)\n",
    "    print(f\"  t+{h}: RMSE={rmse:.2f}, MAE={mae:.2f}\")\n",
    "\n",
    "print(\"\\n✓ All volume models trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf95c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liquidity Classification\n",
    "# Define high/low liquidity based on volume being above/below the 20-day moving average\n",
    "\n",
    "def create_liquidity_labels(df, threshold_quantile=0.5):\n",
    "    \"\"\"\n",
    "    Label each day as high (1) or low (0) liquidity based on rolling median.\n",
    "    \"\"\"\n",
    "    rolling_median = df['volume'].rolling(20).median()\n",
    "    labels = (df['volume'] > rolling_median).astype(int)\n",
    "    return labels\n",
    "\n",
    "# Create liquidity targets for next 5 days\n",
    "liquidity_labels = create_liquidity_labels(features)\n",
    "liquidity_targets = pd.DataFrame(index=features.index)\n",
    "for h in range(1, FORECAST_HORIZON + 1):\n",
    "    liquidity_targets[f'liquidity_t{h}'] = liquidity_labels.shift(-h)\n",
    "\n",
    "# Combine with features\n",
    "liq_df = pd.concat([features, liquidity_targets], axis=1).dropna()\n",
    "\n",
    "liq_X = liq_df[feature_cols]\n",
    "liq_X_train, liq_X_test = liq_X.iloc[:-TEST_DAYS], liq_X.iloc[-TEST_DAYS:]\n",
    "\n",
    "# Train a classifier for each horizon\n",
    "liquidity_models = {}\n",
    "for h in range(1, FORECAST_HORIZON + 1):\n",
    "    target_col = f'liquidity_t{h}'\n",
    "    y_liq = liq_df[target_col]\n",
    "    y_train_liq = y_liq.iloc[:-TEST_DAYS]\n",
    "    y_test_liq = y_liq.iloc[-TEST_DAYS:]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=200, max_depth=8, random_state=42, n_jobs=-1)\n",
    "    clf.fit(liq_X_train, y_train_liq)\n",
    "\n",
    "    acc = clf.score(liq_X_test, y_test_liq)\n",
    "    print(f\"  Liquidity t+{h}: Accuracy={acc:.4f}\")\n",
    "    liquidity_models[h] = clf\n",
    "\n",
    "print(\"\\n✓ Liquidity classifiers trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7970681b",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Evaluation\n",
    "\n",
    "Compute RMSE, MAE, and **Directional Accuracy** for both XGBoost and LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa77e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_accuracy(y_true, y_pred, y_prev):\n",
    "    \"\"\"\n",
    "    Fraction of times the predicted direction (up/down) matches actual direction.\n",
    "    \"\"\"\n",
    "    actual_dir = np.sign(y_true - y_prev)\n",
    "    pred_dir = np.sign(y_pred - y_prev)\n",
    "    return np.mean(actual_dir == pred_dir)\n",
    "\n",
    "\n",
    "# ── XGBoost Evaluation ──\n",
    "print(\"=\"*60)\n",
    "print(\"XGBoost Price Forecast — Test Set Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "xgb_results = []\n",
    "xgb_predictions = pd.DataFrame(index=X_test.index)\n",
    "\n",
    "for h in range(1, FORECAST_HORIZON + 1):\n",
    "    y_true = y_price_test[f'target_close_t{h}'].values\n",
    "    y_pred = xgb_models[h].predict(X_test)\n",
    "    y_prev = X_test['close'].values\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    da = directional_accuracy(y_true, y_pred, y_prev)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    xgb_results.append({'Horizon': f't+{h}', 'RMSE': rmse, 'MAE': mae, 'DA%': da*100, 'MAPE%': mape})\n",
    "    xgb_predictions[f'pred_t{h}'] = y_pred\n",
    "\n",
    "xgb_metrics_df = pd.DataFrame(xgb_results)\n",
    "print(xgb_metrics_df.to_string(index=False))\n",
    "\n",
    "# ── LSTM Evaluation ──\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LSTM Price Forecast — Test Set Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lstm_pred_scaled = lstm_model.predict(X_test_lstm)\n",
    "lstm_pred = scaler_y.inverse_transform(lstm_pred_scaled)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test_lstm)\n",
    "\n",
    "lstm_results = []\n",
    "# Use the previous close from the test set (aligned with the LSTM sequences)\n",
    "prev_close_lstm = X_test.iloc[LOOKBACK:]['close'].values\n",
    "\n",
    "for h in range(FORECAST_HORIZON):\n",
    "    y_true = y_test_actual[:, h]\n",
    "    y_pred = lstm_pred[:, h]\n",
    "    y_prev = prev_close_lstm[:len(y_true)]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    da = directional_accuracy(y_true, y_pred, y_prev)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    lstm_results.append({'Horizon': f't+{h+1}', 'RMSE': rmse, 'MAE': mae, 'DA%': da*100, 'MAPE%': mape})\n",
    "\n",
    "lstm_metrics_df = pd.DataFrame(lstm_results)\n",
    "print(lstm_metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Comparison bar chart ──\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "x = np.arange(FORECAST_HORIZON)\n",
    "width = 0.35\n",
    "\n",
    "for i, metric in enumerate(['RMSE', 'MAE', 'DA%']):\n",
    "    xgb_vals = xgb_metrics_df[metric].values\n",
    "    lstm_vals = lstm_metrics_df[metric].values\n",
    "\n",
    "    axes[i].bar(x - width/2, xgb_vals, width, label='XGBoost', color='#2563eb', alpha=0.8)\n",
    "    axes[i].bar(x + width/2, lstm_vals, width, label='LSTM', color='#f59e0b', alpha=0.8)\n",
    "    axes[i].set_xticks(x)\n",
    "    axes[i].set_xticklabels([f't+{h+1}' for h in range(FORECAST_HORIZON)])\n",
    "    axes[i].set_title(metric, fontweight='bold')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(f'{DEMO_NAME} — XGBoost vs LSTM Comparison', fontweight='bold', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c85aa9",
   "metadata": {},
   "source": [
    "## 9. Visualisation — Prévision vs Réel (avec Intervalles de Confiance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72def466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot forecast vs actual for the last N test days using XGBoost\n",
    "# Pick a window of 30 consecutive test days\n",
    "PLOT_WINDOW = min(30, len(X_test) - FORECAST_HORIZON)\n",
    "\n",
    "fig, axes = plt.subplots(FORECAST_HORIZON, 1, figsize=(14, 4 * FORECAST_HORIZON), sharex=True)\n",
    "\n",
    "for h in range(1, FORECAST_HORIZON + 1):\n",
    "    ax = axes[h-1]\n",
    "    dates = X_test.index[:PLOT_WINDOW]\n",
    "    actual = y_price_test[f'target_close_t{h}'].values[:PLOT_WINDOW]\n",
    "    predicted = xgb_models[h].predict(X_test)[:PLOT_WINDOW]\n",
    "\n",
    "    # Confidence interval (using residual std)\n",
    "    residuals = actual - predicted\n",
    "    std_res = np.std(residuals)\n",
    "\n",
    "    ax.plot(dates, actual, label='Réel', color='#2563eb', linewidth=1.5)\n",
    "    ax.plot(dates, predicted, label='Prévu (XGBoost)', color='#ef4444', linewidth=1.5, linestyle='--')\n",
    "    ax.fill_between(dates,\n",
    "                    predicted - 1.96 * std_res,\n",
    "                    predicted + 1.96 * std_res,\n",
    "                    alpha=0.15, color='#ef4444', label='IC 95%')\n",
    "    ax.set_title(f'Prévision t+{h} jour(s)', fontweight='bold')\n",
    "    ax.set_ylabel('Prix Clôture (TND)')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.suptitle(f'{DEMO_NAME} — Prévision vs Réel (XGBoost)', fontweight='bold', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling 5-day forecast visualization (more realistic: one window ahead)\n",
    "# Shows what a user would see: from today, here are the next 5 predicted prices\n",
    "\n",
    "last_idx = -1  # Use the last available day as \"today\"\n",
    "today_features = X_test.iloc[[last_idx]]\n",
    "today_date = X_test.index[last_idx]\n",
    "\n",
    "# Generate next 5 business days from today\n",
    "future_dates = pd.bdate_range(start=today_date + pd.Timedelta(days=1), periods=FORECAST_HORIZON)\n",
    "\n",
    "# Predictions\n",
    "price_preds = [xgb_models[h].predict(today_features)[0] for h in range(1, FORECAST_HORIZON + 1)]\n",
    "volume_preds = [volume_models[h].predict(today_features)[0] for h in range(1, FORECAST_HORIZON + 1)]\n",
    "liquidity_probs = [liquidity_models[h].predict_proba(today_features)[0][1] for h in range(1, FORECAST_HORIZON + 1)]\n",
    "\n",
    "# Historical context (last 30 days)\n",
    "hist_window = 30\n",
    "hist_dates = X_test.index[max(0, last_idx - hist_window):last_idx + 1]\n",
    "hist_prices = X_test['close'].iloc[max(0, last_idx - hist_window):last_idx + 1]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Price forecast\n",
    "axes[0].plot(hist_dates, hist_prices, color='#2563eb', linewidth=1.5, label='Historique')\n",
    "axes[0].plot(future_dates, price_preds, 'o--', color='#ef4444', linewidth=2, markersize=8, label='Prévision')\n",
    "# Confidence interval\n",
    "std_price = np.std(y_price_test[f'target_close_t1'].values - xgb_models[1].predict(X_test)) * np.sqrt(np.arange(1, FORECAST_HORIZON + 1))\n",
    "axes[0].fill_between(future_dates,\n",
    "                     np.array(price_preds) - 1.96 * std_price,\n",
    "                     np.array(price_preds) + 1.96 * std_price,\n",
    "                     alpha=0.15, color='#ef4444', label='IC 95%')\n",
    "axes[0].axvline(today_date, color='gray', linestyle=':', alpha=0.7, label=\"Aujourd'hui\")\n",
    "axes[0].set_title(f'{DEMO_NAME} — Prévision Prix de Clôture (5 jours)', fontweight='bold')\n",
    "axes[0].set_ylabel('Prix (TND)')\n",
    "axes[0].legend()\n",
    "\n",
    "# Volume forecast\n",
    "axes[1].bar(future_dates, volume_preds, width=0.8, color='#10b981', alpha=0.7)\n",
    "axes[1].set_title('Prévision Volume', fontweight='bold')\n",
    "axes[1].set_ylabel('Quantité')\n",
    "\n",
    "# Liquidity probability\n",
    "colors = ['#10b981' if p > 0.5 else '#ef4444' for p in liquidity_probs]\n",
    "axes[2].bar(future_dates, liquidity_probs, width=0.8, color=colors, alpha=0.7)\n",
    "axes[2].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "axes[2].set_title('Probabilité de Liquidité Élevée', fontweight='bold')\n",
    "axes[2].set_ylabel('Probabilité')\n",
    "axes[2].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print predictions table\n",
    "pred_df = pd.DataFrame({\n",
    "    'Date': future_dates.strftime('%Y-%m-%d'),\n",
    "    'Prix Clôture Prévu': [f'{p:.3f} TND' for p in price_preds],\n",
    "    'Volume Prévu': [f'{int(v):,}' for v in volume_preds],\n",
    "    'Prob. Liquidité Haute': [f'{p:.1%}' for p in liquidity_probs],\n",
    "})\n",
    "print(\"\\nPrévisions depuis\", today_date.strftime('%Y-%m-%d'), \":\")\n",
    "print(pred_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bd82c8",
   "metadata": {},
   "source": [
    "## 10. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a1abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for t+1 price model\n",
    "importance = xgb_models[1].feature_importances_\n",
    "feat_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importance\n",
    "}).sort_values('importance', ascending=True).tail(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.barh(feat_importance['feature'], feat_importance['importance'], color='#2563eb', alpha=0.8)\n",
    "ax.set_title(f'{DEMO_NAME} — Top 20 Feature Importances (t+1)', fontweight='bold')\n",
    "ax.set_xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877bf647",
   "metadata": {},
   "source": [
    "## 11. Export Models for the Forecasting Service\n",
    "\n",
    "Save models and scalers so the FastAPI service can load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "MODEL_DIR = os.path.join('..', 'backend', 'services', 'forecasting', 'models')\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Save XGBoost price models\n",
    "for h in range(1, FORECAST_HORIZON + 1):\n",
    "    path = os.path.join(MODEL_DIR, f'xgb_price_t{h}.json')\n",
    "    xgb_models[h].save_model(path)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "# Save XGBoost volume models\n",
    "for h in range(1, FORECAST_HORIZON + 1):\n",
    "    path = os.path.join(MODEL_DIR, f'xgb_volume_t{h}.json')\n",
    "    volume_models[h].save_model(path)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "# Save liquidity classifiers\n",
    "for h in range(1, FORECAST_HORIZON + 1):\n",
    "    path = os.path.join(MODEL_DIR, f'rf_liquidity_t{h}.joblib')\n",
    "    joblib.dump(liquidity_models[h], path)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "# Save LSTM model\n",
    "lstm_path = os.path.join(MODEL_DIR, 'lstm_price.keras')\n",
    "lstm_model.save(lstm_path)\n",
    "print(f\"Saved: {lstm_path}\")\n",
    "\n",
    "# Save scalers\n",
    "joblib.dump(scaler_X, os.path.join(MODEL_DIR, 'scaler_X.joblib'))\n",
    "joblib.dump(scaler_y, os.path.join(MODEL_DIR, 'scaler_y.joblib'))\n",
    "print(\"Saved: scalers\")\n",
    "\n",
    "# Save feature columns list\n",
    "joblib.dump(feature_cols, os.path.join(MODEL_DIR, 'feature_cols.joblib'))\n",
    "print(\"Saved: feature column names\")\n",
    "\n",
    "# Save training metadata\n",
    "import json\n",
    "metadata = {\n",
    "    'demo_stock_code': DEMO_CODE,\n",
    "    'demo_stock_name': DEMO_NAME,\n",
    "    'forecast_horizon': FORECAST_HORIZON,\n",
    "    'lookback_lstm': LOOKBACK,\n",
    "    'n_features': len(feature_cols),\n",
    "    'train_end': str(X_train.index.max()),\n",
    "    'test_start': str(X_test.index.min()),\n",
    "    'xgb_metrics': xgb_results,\n",
    "    'lstm_metrics': lstm_results,\n",
    "}\n",
    "with open(os.path.join(MODEL_DIR, 'metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "print(\"Saved: metadata.json\")\n",
    "\n",
    "print(\"\\n✅ All models and artifacts exported successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
